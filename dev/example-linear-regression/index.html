<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Linear regression Â· SossMLJ.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">SossMLJ.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">Examples</span><ul><li class="is-active"><a class="tocitem" href>Linear regression</a></li><li><a class="tocitem" href="../example-multinomial-logistic-regression/">Multinomial logistic regression</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>Linear regression</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Linear regression</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/cscherrer/SossMLJ.jl/blob/master/examples/example-linear-regression.jl" title="Edit on GitHub"><span class="docs-icon fab">ï‚›</span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Linear-regression"><a class="docs-heading-anchor" href="#Linear-regression">Linear regression</a><a id="Linear-regression-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-regression" title="Permalink"></a></h1><p>Import the necessary packages:</p><pre><code class="language-julia">using Distributions
using MLJBase
using Soss
using SossMLJ
using Statistics</code></pre><p>In this example, we fit a Bayesian linear regression model with the canonical link function.</p><p>Suppose that we are given a matrix of features <code>X</code> and a column vector of labels <code>y</code>. <code>X</code> has <code>n</code> rows and <code>p</code> columns. <code>y</code> has <code>n</code> elements. We assume that our observation vector <code>y</code> is a realization of a random variable <code>Y</code>. We define <code>Î¼</code> (mu) as the expected value of <code>Y</code>, i.e. <code>Î¼ := E[Y]</code>. Our model comprises three components:</p><ol><li>The probability distribution of <code>Y</code>: for linear regression, we assume that each <code>Yáµ¢</code> follows a normal distribution with mean <code>Î¼áµ¢</code> and variance <code>ÏƒÂ²</code>.</li><li>The systematic component, which consists of linear predictor <code>Î·</code> (eta), which we define as <code>Î· := XÎ²</code>, where <code>Î²</code> is the column vector of <code>p</code> coefficients.</li><li>The link function <code>g</code>, which provides the following relationship: <code>g(E[Y]) = g(Î¼) = Î· = XÎ²</code>. It follows that <code>Î¼ = gâ»Â¹(Î·)</code>, where <code>gâ»Â¹</code> denotes the inverse of <code>g</code>. For linear regression, the canonical link function is the identity function. Therefore, when using the canonical link function, <code>Î¼ = gâ»Â¹(Î·) = Î·</code>.</li></ol><p>In this model, the parameters that we want to estimate are <code>Î²</code> and <code>Ïƒ</code>. We need to select prior distributions for these parameters. For each <code>Î²áµ¢</code> we choose a normal distribution with zero mean and variance <code>sÂ²</code>. Here, <code>Î²áµ¢</code> denotes the <code>i</code>th component of <code>Î²</code>. For <code>Ïƒ</code>, we will choose a half-normal distribution with variance <code>tÂ²</code>. <code>s</code> and <code>t</code> are hyperparameters that we will need to choose.</p><p>We define this model using the Soss probabilistic programming library:</p><pre><code class="language-julia">m = @model X, s, t begin
    p = size(X, 2) # number of features
    Î² ~ Normal(0, s) |&gt; iid(p) # coefficients
    Ïƒ ~ HalfNormal(t) # dispersion
    Î· = X * Î² # linear predictor
    Î¼ = Î· # `Î¼ = gâ»Â¹(Î·) = Î·`
    y ~ For(eachindex(Î¼)) do j
        Normal(Î¼[j], Ïƒ) # `Yáµ¢ ~ Normal(mean=Î¼áµ¢, variance=ÏƒÂ²)`
    end
end;</code></pre><p>Generate some synthetic features. Let us generate two continuous features and two binary categorical features.</p><pre><code class="language-julia">num_rows = 1_000
x1 = randn(num_rows)
x2 = randn(num_rows)
x3 = Int.(rand(num_rows) .&gt; 0.5)
x4 = Int.(rand(num_rows) .&gt; 0.5)
X = (x1 = x1, x2 = x2, x3 = x3, x4 = x4)</code></pre><pre class="documenter-example-output">(x1 = [-0.7928323332216676, 0.08869955452503346, 0.2642342539255355, -1.258652437099135, -2.0687909565798703, -0.2706952306740847, -1.5620670774049175, -0.4885248538328402, -0.12030904619222721, -1.3082361644286984  â€¦  1.7846831373359484, 0.8591017579640039, -0.4157458033685822, -1.413291383653098, -0.5680540099513229, -0.5971743076857059, -0.14645636082510147, 0.2242951048630291, -0.2725535791648264, 1.079328156781994],
 x2 = [-0.09124514839927989, 0.3303969068044855, -0.15543307491838346, -0.10140805734127811, 0.24744764957726836, -1.3274409451494527, 0.27873446662865936, 0.7666033391708269, 0.21176979580879252, -0.7114366058761238  â€¦  -0.9580722446244619, -1.3960898015215026, -0.3278525332164945, -0.694375942771906, 0.27499256579156867, -1.0393656630070622, -0.8400637492984239, -1.0172576640775854, -0.014542255889615378, -1.9360209160789252],
 x3 = [1, 0, 0, 1, 1, 0, 0, 0, 0, 1  â€¦  0, 1, 0, 1, 1, 1, 0, 1, 0, 1],
 x4 = [1, 0, 0, 0, 0, 1, 1, 1, 1, 0  â€¦  0, 1, 0, 1, 1, 1, 1, 0, 1, 0],)</pre><p>Define the hyperparameters of our prior distributions:</p><pre><code class="language-julia">hyperparams = (s=0.1, t=0.1)</code></pre><pre class="documenter-example-output">(s = 0.1,
 t = 0.1,)</pre><p>Convert the Soss model into a <code>SossMLJModel</code>:</p><pre><code class="language-julia">model = SossMLJModel(;
    model       = m,
    hyperparams = hyperparams,
    infer       = dynamicHMC,
    response    = :y,
);</code></pre><p>Generate some synthetic labels:</p><pre><code class="language-julia">args = merge(model.transform(X), hyperparams)
truth = rand(m(args))</code></pre><pre class="documenter-example-output">(p = 4,
 Î· = [0.0011479409289402078, 0.006394603112552303, 0.0022729181878626045, -0.014973253723524833, -0.023821777254702893, -0.016281802658499305, -0.014928703441261884, 0.010783315259661625, 0.008971742593628988, -0.02480895552006257  â€¦  0.016702257313955533, 0.010454203519173987, -0.01200229532883407, -0.018440104170269284, 0.010418989500105497, -0.009431476060081767, -0.006962949698637538, -0.0028577232141236922, 0.0030124554541950824, -0.0016235165267884516],
 Î¼ = [0.0011479409289402078, 0.006394603112552303, 0.0022729181878626045, -0.014973253723524833, -0.023821777254702893, -0.016281802658499305, -0.014928703441261884, 0.010783315259661625, 0.008971742593628988, -0.02480895552006257  â€¦  0.016702257313955533, 0.010454203519173987, -0.01200229532883407, -0.018440104170269284, 0.010418989500105497, -0.009431476060081767, -0.006962949698637538, -0.0028577232141236922, 0.0030124554541950824, -0.0016235165267884516],
 Ïƒ = 0.016809080399002262,
 Î² = [0.01726100305160752, 0.014720355218390452, 0.0082451124602271, 0.007931070788257572],
 y = [-0.02113448860861107, 0.013601911872705038, -0.01544120088124149, -0.018035624191651563, -0.024475334904285932, -0.0048926882687986804, 0.007256881357502432, -0.006087407637008152, 0.02084088563263381, -0.026404938621043678  â€¦  0.023199107365805702, 0.007781828425193598, 0.006731221603758955, -0.020789122906913992, -0.0030671138125499906, 0.0015555438069541905, 0.006993673227508112, 0.021582853073483073, -0.005696161003970495, 0.03274379109262061],)</pre><p>Create an MLJ machine for fitting our model:</p><pre><code class="language-julia">mach = MLJBase.machine(model, X, truth.y)</code></pre><pre class="documenter-example-output">[34mMachine{SossMLJModel{,â€¦}} @421[39m trained 0 times.
  args: 
    1:	[34mSource @507[39m â `ScientificTypes.Table{Union{AbstractVector{ScientificTypes.Continuous}, AbstractVector{ScientificTypes.Count}}}`
    2:	[34mSource @669[39m â `AbstractVector{ScientificTypes.Continuous}`
</pre><p>Fit the machine. This may take several minutes.</p><pre><code class="language-julia">fit!(mach)</code></pre><pre class="documenter-example-output">[34mMachine{SossMLJModel{,â€¦}} @421[39m trained 1 time.
  args: 
    1:	[34mSource @507[39m â `ScientificTypes.Table{Union{AbstractVector{ScientificTypes.Continuous}, AbstractVector{ScientificTypes.Count}}}`
    2:	[34mSource @669[39m â `AbstractVector{ScientificTypes.Continuous}`
</pre><p>Construct the posterior distribution and the joint posterior predictive distribution:</p><pre><code class="language-julia">predictor_joint = predict_joint(mach, X)
typeof(predictor_joint)</code></pre><pre class="documenter-example-output">SossMLJ.SossMLJPredictor{SossMLJModel{SossMLJ.SossMLJPredictor, Soss.Model{NamedTuple{(:X, :s, :t), T} where T&lt;:Tuple, TypeEncoding(begin
    Ïƒ ~ HalfNormal(t)
    p = size(X, 2)
    Î² ~ Normal(0, s) |&gt; iid(p)
    Î· = X * Î²
    Î¼ = Î·
    y ~ For(eachindex(Î¼)) do j
            Normal(Î¼[j], Ïƒ)
        end
end), TypeEncoding(Main.ex-example-linear-regression)}, NamedTuple{(:s, :t), Tuple{Float64, Float64}}, typeof(Soss.dynamicHMC), Symbol, typeof(SossMLJ.default_transform)}, Vector{NamedTuple{(:Ïƒ, :Î²), Tuple{Float64, Vector{Float64}}}}, Soss.Model{NamedTuple{(:X, :Ïƒ, :Î²), T} where T&lt;:Tuple, TypeEncoding(begin
    Î· = X * Î²
    Î¼ = Î·
    y ~ For(eachindex(Î¼)) do j
            Normal(Î¼[j], Ïƒ)
        end
end), TypeEncoding(Main.ex-example-linear-regression)}, NamedTuple{(:X, :s, :t), Tuple{Matrix{Float64}, Float64, Float64}}}</pre><p>Draw a single sample from the joint posterior predictive distribution:</p><pre><code class="language-julia">single_sample = rand(predictor_joint; response = :y)</code></pre><pre class="documenter-example-output">1000-element Vector{Float64}:
 -0.004205044248922946
 -0.01777339654309875
  0.013795744561872544
 -0.025884280497236935
 -0.029435478557348606
 -0.024091788851530398
 -0.004473531928833726
 -0.007676283739652276
  0.028836466875902512
  0.001193574192037243
  â‹®
 -0.0039355399717032075
  0.022437254770923627
  0.003927320981374136
 -0.009863919625908804
  0.0004360814002562427
 -0.04850822191362942
 -0.028854499811970642
  0.021706831147700823
 -0.03526724936373375</pre><p>Evaluate the logpdf of the joint posterior predictive distribution at this sample:</p><pre><code class="language-julia">logpdf(predictor_joint, single_sample)</code></pre><pre class="documenter-example-output">2619.860995292669</pre><p>True <code>Î²</code>:</p><pre><code class="language-julia">truth.Î²</code></pre><pre class="documenter-example-output">4-element Vector{Float64}:
 0.01726100305160752
 0.014720355218390452
 0.0082451124602271
 0.007931070788257572</pre><p>Posterior distribution of <code>Î²</code></p><pre><code class="language-julia">predict_particles(mach, X; response = :Î²)</code></pre><pre class="documenter-example-output">4-element Vector{MonteCarloMeasurements.Particles{Float64, 1000}}:
 0.0172 Â± 0.00048
 0.015 Â± 0.0005
 0.00759 Â± 0.00084
 0.00798 Â± 0.00086</pre><p>Difference between the posterior distribution of <code>Î²</code> to the true values:</p><pre><code class="language-julia">truth.Î² - predict_particles(mach, X; response = :Î²)</code></pre><pre class="documenter-example-output">4-element Vector{MonteCarloMeasurements.Particles{Float64, 1000}}:
  3.57e-5 Â± 0.00048
 -0.00031 Â± 0.0005
  0.000653 Â± 0.00084
 -4.81e-5 Â± 0.00086</pre><p>Compare the joint posterior predictive distribution of <code>Î¼</code> to the true values:</p><pre><code class="language-julia">truth.Î¼ - predict_particles(mach, X; response = :Î¼)</code></pre><pre class="documenter-example-output">1000-element Vector{MonteCarloMeasurements.Particles{Float64, 1000}}:
  0.000605 Â± 0.00094
 -9.92e-5 Â± 0.00017
  5.76e-5 Â± 0.00015
  0.000639 Â± 0.001
  0.000502 Â± 0.0013
  0.000354 Â± 0.0011
 -0.00019 Â± 0.0012
 -0.000303 Â± 0.00096
 -0.000118 Â± 0.00087
  0.000826 Â± 0.0011
  â‹®
  0.00107 Â± 0.0012
  8.68e-5 Â± 0.00027
  0.000769 Â± 0.0012
  0.000499 Â± 0.0009
  0.000905 Â± 0.0011
  0.000207 Â± 0.00097
  0.000976 Â± 0.001
 -5.33e-5 Â± 0.00087
  0.00129 Â± 0.0014</pre><p>Compare the joint posterior predictive distribution of <code>y</code> to the true values:</p><pre><code class="language-julia">truth.y - predict_particles(mach, X; response = :y)</code></pre><pre class="documenter-example-output">1000-element Vector{MonteCarloMeasurements.Particles{Float64, 1000}}:
 -0.0217 Â± 0.017
  0.00711 Â± 0.017
 -0.0176 Â± 0.017
 -0.00243 Â± 0.017
 -0.000154 Â± 0.017
  0.0117 Â± 0.017
  0.022 Â± 0.017
 -0.0172 Â± 0.017
  0.0117 Â± 0.017
 -0.000747 Â± 0.017
  â‹®
 -0.0016 Â± 0.017
  0.0188 Â± 0.017
 -0.00157 Â± 0.017
 -0.013 Â± 0.017
  0.0119 Â± 0.017
  0.0142 Â± 0.017
  0.0254 Â± 0.017
 -0.00875 Â± 0.017
  0.0357 Â± 0.017</pre><p>Construct each of the marginal posterior predictive distributions:</p><pre><code class="language-julia">predictor_marginal = MLJBase.predict(mach, X)
typeof(predictor_marginal)</code></pre><pre class="documenter-example-output">Vector{SossMLJ.SossMLJPredictor{SossMLJModel{SossMLJ.SossMLJPredictor, Soss.Model{NamedTuple{(:X, :s, :t), T} where T&lt;:Tuple, TypeEncoding(begin
    Ïƒ ~ HalfNormal(t)
    p = size(X, 2)
    Î² ~ Normal(0, s) |&gt; iid(p)
    Î· = X * Î²
    Î¼ = Î·
    y ~ For(eachindex(Î¼)) do j
            Normal(Î¼[j], Ïƒ)
        end
end), TypeEncoding(Main.ex-example-linear-regression)}, NamedTuple{(:s, :t), Tuple{Float64, Float64}}, typeof(Soss.dynamicHMC), Symbol, typeof(SossMLJ.default_transform)}, Vector{NamedTuple{(:Ïƒ, :Î²), Tuple{Float64, Vector{Float64}}}}, Soss.Model{NamedTuple{(:X, :Ïƒ, :Î²), T} where T&lt;:Tuple, TypeEncoding(begin
    Î· = X * Î²
    Î¼ = Î·
    y ~ For(eachindex(Î¼)) do j
            Normal(Î¼[j], Ïƒ)
        end
end), TypeEncoding(Main.ex-example-linear-regression)}, NamedTuple{(:X, :s, :t), Tuple{Matrix{Float64}, Float64, Float64}}}} (alias for Array{SossMLJ.SossMLJPredictor{SossMLJModel{SossMLJ.SossMLJPredictor, Soss.Model{NamedTuple{(:X, :s, :t), T} where T&lt;:Tuple, TypeEncoding(begin
    Ïƒ ~ HalfNormal(t)
    p = size(X, 2)
    Î² ~ Normal(0, s) |&gt; iid(p)
    Î· = X * Î²
    Î¼ = Î·
    y ~ For(eachindex(Î¼)) do j
            Normal(Î¼[j], Ïƒ)
        end
end), TypeEncoding(Main.ex-example-linear-regression)}, NamedTuple{(:s, :t), Tuple{Float64, Float64}}, typeof(Soss.dynamicHMC), Symbol, typeof(SossMLJ.default_transform)}, Array{NamedTuple{(:Ïƒ, :Î²), Tuple{Float64, Array{Float64, 1}}}, 1}, Soss.Model{NamedTuple{(:X, :Ïƒ, :Î²), T} where T&lt;:Tuple, TypeEncoding(begin
    Î· = X * Î²
    Î¼ = Î·
    y ~ For(eachindex(Î¼)) do j
            Normal(Î¼[j], Ïƒ)
        end
end), TypeEncoding(Main.ex-example-linear-regression)}, NamedTuple{(:X, :s, :t), Tuple{Array{Float64, 2}, Float64, Float64}}}, 1})</pre><p><code>predictor_marginal</code> has one element for each row in <code>X</code></p><pre><code class="language-julia">size(predictor_marginal)</code></pre><pre class="documenter-example-output">(1000,)</pre><p>Draw a single sample from each of the marginal posterior predictive distributions:</p><pre><code class="language-julia">only.(rand.(predictor_marginal))</code></pre><pre class="documenter-example-output">1000-element Vector{Float64}:
  0.014019071386215637
  0.019705436837517794
 -0.023549355744404997
 -0.013077509014857441
 -0.03296411139194573
 -0.00689836693785819
 -0.032513197462030685
  0.014554029348867068
  0.01805200935837424
 -0.037376926897326516
  â‹®
  0.023800401145206613
  0.007797363001057801
 -0.015805010307221548
  0.01973338068338997
 -0.03331026814393388
 -0.016063322386153777
  1.7452920376245535e-5
  0.0007042789931651161
 -0.018088203196293326</pre><p>Use cross-validation to evaluate the model with respect to the expected value of the root mean square error (RMSE)</p><pre><code class="language-julia">evaluate!(mach, resampling=CV(; nfolds = 6, shuffle = true), measure=rms_expected, operation=predict_particles)</code></pre><pre class="documenter-example-output">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚ _.measure        â”‚ _.measurement â”‚ _.per_fold                                â‹¯
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚ RMSExpected @226 â”‚ 0.0239        â”‚ [0.0258, 0.0233, 0.024, 0.0234, 0.0243, 0 â‹¯
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                                                                1 column omitted
_.per_observation = [missing]
_.fitted_params_per_fold = [ â€¦ ]
_.report_per_fold = [ â€¦ ]
</pre><p>Use cross-validation to evaluate the model with respect to the median of the root mean square error (RMSE)</p><pre><code class="language-julia">evaluate!(mach, resampling=CV(; nfolds = 6, shuffle = true), measure=rms_median, operation=predict_particles)</code></pre><pre class="documenter-example-output">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚ _.measure      â”‚ _.measurement â”‚ _.per_fold                                  â‹¯
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚ RMSMedian @905 â”‚ 0.0239        â”‚ [0.024, 0.0243, 0.0236, 0.0239, 0.0238, 0.0 â‹¯
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                                                                1 column omitted
_.per_observation = [missing]
_.fitted_params_per_fold = [ â€¦ ]
_.report_per_fold = [ â€¦ ]
</pre><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">Â« Home</a><a class="docs-footer-nextpage" href="../example-multinomial-logistic-regression/">Multinomial logistic regression Â»</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 29 March 2021 05:57">Monday 29 March 2021</span>. Using Julia version 1.6.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
