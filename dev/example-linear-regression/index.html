<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Linear regression ¬∑ SossMLJ.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">SossMLJ.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">Examples</span><ul><li class="is-active"><a class="tocitem" href>Linear regression</a></li><li><a class="tocitem" href="../example-multinomial-logistic-regression/">Multinomial logistic regression</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>Linear regression</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Linear regression</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/cscherrer/SossMLJ.jl/blob/master/examples/example-linear-regression.jl" title="Edit on GitHub"><span class="docs-icon fab">ÔÇõ</span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Linear-regression"><a class="docs-heading-anchor" href="#Linear-regression">Linear regression</a><a id="Linear-regression-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-regression" title="Permalink"></a></h1><p>Import the necessary packages:</p><pre><code class="language-julia">using Distributions
using MLJBase
using Soss
using SossMLJ
using Statistics</code></pre><p>In this example, we fit a Bayesian linear regression model with the canonical link function.</p><p>Suppose that we are given a matrix of features <code>X</code> and a column vector of labels <code>y</code>. <code>X</code> has <code>n</code> rows and <code>p</code> columns. <code>y</code> has <code>n</code> elements. We assume that our observation vector <code>y</code> is a realization of a random variable <code>Y</code>. We define <code>Œº</code> (mu) as the expected value of <code>Y</code>, i.e. <code>Œº := E[Y]</code>. Our model comprises three components:</p><ol><li>The probability distribution of <code>Y</code>: for linear regression, we assume that each <code>Y·µ¢</code> follows a normal distribution with mean <code>Œº·µ¢</code> and variance <code>œÉ¬≤</code>.</li><li>The systematic component, which consists of linear predictor <code>Œ∑</code> (eta), which we define as <code>Œ∑ := XŒ≤</code>, where <code>Œ≤</code> is the column vector of <code>p</code> coefficients.</li><li>The link function <code>g</code>, which provides the following relationship: <code>g(E[Y]) = g(Œº) = Œ∑ = XŒ≤</code>. It follows that <code>Œº = g‚Åª¬π(Œ∑)</code>, where <code>g‚Åª¬π</code> denotes the inverse of <code>g</code>. For linear regression, the canonical link function is the identity function. Therefore, when using the canonical link function, <code>Œº = g‚Åª¬π(Œ∑) = Œ∑</code>.</li></ol><p>In this model, the parameters that we want to estimate are <code>Œ≤</code> and <code>œÉ</code>. We need to select prior distributions for these parameters. For each <code>Œ≤·µ¢</code> we choose a normal distribution with zero mean and variance <code>s¬≤</code>. Here, <code>Œ≤·µ¢</code> denotes the <code>i</code>th component of <code>Œ≤</code>. For <code>œÉ</code>, we will choose a half-normal distribution with variance <code>t¬≤</code>. <code>s</code> and <code>t</code> are hyperparameters that we will need to choose.</p><p>We define this model using the Soss probabilistic programming library:</p><pre><code class="language-julia">m = @model X, s, t begin
    p = size(X, 2) # number of features
    Œ≤ ~ Normal(0, s) |&gt; iid(p) # coefficients
    œÉ ~ HalfNormal(t) # dispersion
    Œ∑ = X * Œ≤ # linear predictor
    Œº = Œ∑ # `Œº = g‚Åª¬π(Œ∑) = Œ∑`
    y ~ For(eachindex(Œº)) do j
        Normal(Œº[j], œÉ) # `Y·µ¢ ~ Normal(mean=Œº·µ¢, variance=œÉ¬≤)`
    end
end;</code></pre><p>Generate some synthetic features. Let us generate two continuous features and two binary categorical features.</p><pre><code class="language-julia">num_rows = 1_000
x1 = randn(num_rows)
x2 = randn(num_rows)
x3 = Int.(rand(num_rows) .&gt; 0.5)
x4 = Int.(rand(num_rows) .&gt; 0.5)
X = (x1 = x1, x2 = x2, x3 = x3, x4 = x4)</code></pre><pre class="documenter-example-output">(x1 = [-0.22086537336494344, -1.0393401765039576, 0.2645863545878687, 0.6401666706867258, -0.5896973126191221, -0.738527849878872, -0.592892426005407, -2.156448494850359, 1.3410111018186277, -0.2454228167624639  ‚Ä¶  0.675696653766816, -0.32594529057200355, -0.21141036402513422, 2.058668841627336, -1.2524334229744591, -1.8418336055357982, 2.0869969850684638, -1.9440174445913818, -0.9790542929635694, 0.104275958199314],
 x2 = [0.7223948717170937, 0.7827317124073127, 1.18210857938978, 1.2691902907194403, -0.04166912640762065, -0.998395438717675, 0.455434652752246, 0.07933337182345929, -0.3063341032313149, 1.0855864092537524  ‚Ä¶  0.5306423561368502, -0.5399939168588398, -0.03415755286395063, -0.1778242498531976, -1.4589592289183697, 0.3392634211178991, -0.835606608836131, 1.2322243840020346, -1.5705995250922329, -1.382835793539853],
 x3 = [1, 0, 1, 1, 1, 1, 1, 1, 1, 0  ‚Ä¶  0, 0, 0, 0, 0, 0, 1, 0, 0, 0],
 x4 = [1, 1, 1, 0, 1, 0, 0, 1, 1, 0  ‚Ä¶  0, 0, 0, 1, 0, 1, 1, 1, 0, 1],)</pre><p>Define the hyperparameters of our prior distributions:</p><pre><code class="language-julia">hyperparams = (s=0.1, t=0.1)</code></pre><pre class="documenter-example-output">(s = 0.1,
 t = 0.1,)</pre><p>Convert the Soss model into a <code>SossMLJModel</code>:</p><pre><code class="language-julia">model = SossMLJModel(;
    model       = m,
    hyperparams = hyperparams,
    infer       = dynamicHMC,
    response    = :y,
);</code></pre><p>Generate some synthetic labels:</p><pre><code class="language-julia">args = merge(model.transform(X), hyperparams)
truth = rand(m(args))</code></pre><pre class="documenter-example-output">(p = 4,
 Œ∑ = [0.21672891298650948, -0.01965095147981237, 0.2914616185245819, 0.3032051522793495, 0.13385171345289815, 0.030436324296336026, 0.13565066588157743, -0.006377929541295582, 0.2993864122913016, 0.04514395210132605  ‚Ä¶  0.09714740278454555, -0.06473230342090776, -0.022098368451491827, 0.21224944071855573, -0.20997749063125687, -0.1232784384850013, 0.3364789841047335, -0.07673764070135455, -0.19120486276417176, -0.04798900217597511],
 Œº = [0.21672891298650948, -0.01965095147981237, 0.2914616185245819, 0.3032051522793495, 0.13385171345289815, 0.030436324296336026, 0.13565066588157743, -0.006377929541295582, 0.2993864122913016, 0.04514395210132605  ‚Ä¶  0.09714740278454555, -0.06473230342090776, -0.022098368451491827, 0.21224944071855573, -0.20997749063125687, -0.1232784384850013, 0.3364789841047335, -0.07673764070135455, -0.19120486276417176, -0.04798900217597511],
 œÉ = 0.017557122071195815,
 Œ≤ = [0.09436268866129793, 0.06291780034056751, 0.16294264273632578, 0.029176224407335208],
 y = [0.19792291209915808, -0.007668695050723812, 0.30045620309167176, 0.3114783694384989, 0.11537413164446608, 0.011494427093799442, 0.13424151645150595, 0.0072278650232873755, 0.3232621340206863, 0.07670178321897593  ‚Ä¶  0.09967813708250907, -0.06186057256109908, -0.03672575451066832, 0.2029275016652778, -0.19484046127330698, -0.10231308066709738, 0.3319192116190784, -0.07188394328122655, -0.1949409448076215, -0.05835102616193956],)</pre><p>Create an MLJ machine for fitting our model:</p><pre><code class="language-julia">mach = MLJBase.machine(model, X, truth.y)</code></pre><pre class="documenter-example-output">[34mMachine{SossMLJModel{,‚Ä¶}} @165[39m trained 0 times.
  args: 
    1:	[34mSource @403[39m ‚èé `ScientificTypes.Table{Union{AbstractVector{ScientificTypes.Continuous}, AbstractVector{ScientificTypes.Count}}}`
    2:	[34mSource @632[39m ‚èé `AbstractVector{ScientificTypes.Continuous}`
</pre><p>Fit the machine. This may take several minutes.</p><pre><code class="language-julia">fit!(mach)</code></pre><pre class="documenter-example-output">[34mMachine{SossMLJModel{,‚Ä¶}} @165[39m trained 1 time.
  args: 
    1:	[34mSource @403[39m ‚èé `ScientificTypes.Table{Union{AbstractVector{ScientificTypes.Continuous}, AbstractVector{ScientificTypes.Count}}}`
    2:	[34mSource @632[39m ‚èé `AbstractVector{ScientificTypes.Continuous}`
</pre><p>Construct the posterior distribution and the joint posterior predictive distribution:</p><pre><code class="language-julia">predictor_joint = predict_joint(mach, X)
typeof(predictor_joint)</code></pre><pre class="documenter-example-output">SossMLJ.SossMLJPredictor{SossMLJModel{SossMLJ.SossMLJPredictor, Soss.Model{NamedTuple{(:X, :s, :t), T} where T&lt;:Tuple, TypeEncoding(begin
    œÉ ~ HalfNormal(t)
    p = size(X, 2)
    Œ≤ ~ Normal(0, s) |&gt; iid(p)
    Œ∑ = X * Œ≤
    Œº = Œ∑
    y ~ For(eachindex(Œº)) do j
            Normal(Œº[j], œÉ)
        end
end), TypeEncoding(Main.ex-example-linear-regression)}, NamedTuple{(:s, :t), Tuple{Float64, Float64}}, typeof(Soss.dynamicHMC), Symbol, typeof(SossMLJ.default_transform)}, Vector{NamedTuple{(:œÉ, :Œ≤), Tuple{Float64, Vector{Float64}}}}, Soss.Model{NamedTuple{(:X, :œÉ, :Œ≤), T} where T&lt;:Tuple, TypeEncoding(begin
    Œ∑ = X * Œ≤
    Œº = Œ∑
    y ~ For(eachindex(Œº)) do j
            Normal(Œº[j], œÉ)
        end
end), TypeEncoding(Main.ex-example-linear-regression)}, NamedTuple{(:X, :s, :t), Tuple{Matrix{Float64}, Float64, Float64}}}</pre><p>Draw a single sample from the joint posterior predictive distribution:</p><pre><code class="language-julia">single_sample = rand(predictor_joint; response = :y)</code></pre><pre class="documenter-example-output">1000-element Vector{Float64}:
  0.23317093229893057
 -0.011727701037232434
  0.2827935906376695
  0.31380583130459505
  0.12585152253530604
  0.03929893142742991
  0.13846936940875468
 -0.03462629753604405
  0.29473308373790835
  0.03737341125553342
  ‚ãÆ
 -0.041502648838691446
 -0.00934116048384099
  0.180998125256057
 -0.21547590714063175
 -0.11918525685722628
  0.33685418010016005
 -0.0859002748483669
 -0.18470525379873287
 -0.06849073130500591</pre><p>Evaluate the logpdf of the joint posterior predictive distribution at this sample:</p><pre><code class="language-julia">logpdf(predictor_joint, single_sample)</code></pre><pre class="documenter-example-output">2646.158497245938</pre><p>True <code>Œ≤</code>:</p><pre><code class="language-julia">truth.Œ≤</code></pre><pre class="documenter-example-output">4-element Vector{Float64}:
 0.09436268866129793
 0.06291780034056751
 0.16294264273632578
 0.029176224407335208</pre><p>Posterior distribution of <code>Œ≤</code></p><pre><code class="language-julia">predict_particles(mach, X; response = :Œ≤)</code></pre><pre class="documenter-example-output">4-element Vector{MonteCarloMeasurements.Particles{Float64, 1000}}:
 0.0946 ¬± 0.00054
 0.0626 ¬± 0.00058
 0.163 ¬± 0.00087
 0.0292 ¬± 0.00089</pre><p>Difference between the posterior distribution of <code>Œ≤</code> to the true values:</p><pre><code class="language-julia">truth.Œ≤ - predict_particles(mach, X; response = :Œ≤)</code></pre><pre class="documenter-example-output">4-element Vector{MonteCarloMeasurements.Particles{Float64, 1000}}:
 -0.000223 ¬± 0.00054
  0.00032 ¬± 0.00058
  0.000244 ¬± 0.00087
 -3.69e-5 ¬± 0.00089</pre><p>Compare the joint posterior predictive distribution of <code>Œº</code> to the true values:</p><pre><code class="language-julia">truth.Œº - predict_particles(mach, X; response = :Œº)</code></pre><pre class="documenter-example-output">1000-element Vector{MonteCarloMeasurements.Particles{Float64, 1000}}:
  0.000487 ¬± 0.001
  0.000446 ¬± 0.0011
  0.000526 ¬± 0.0012
  0.000507 ¬± 0.0012
  0.000325 ¬± 0.001
  8.9e-5 ¬± 0.0012
  0.000522 ¬± 0.00097
  0.000713 ¬± 0.0015
 -0.000191 ¬± 0.0012
  0.000402 ¬± 0.00064
  ‚ãÆ
 -0.0001 ¬± 0.00037
  3.63e-5 ¬± 0.00012
 -0.000553 ¬± 0.0014
 -0.000187 ¬± 0.0011
  0.000483 ¬± 0.0013
 -0.000526 ¬± 0.0015
  0.000791 ¬± 0.0015
 -0.000284 ¬± 0.0011
 -0.000503 ¬± 0.0012</pre><p>Compare the joint posterior predictive distribution of <code>y</code> to the true values:</p><pre><code class="language-julia">truth.y - predict_particles(mach, X; response = :y)</code></pre><pre class="documenter-example-output">1000-element Vector{MonteCarloMeasurements.Particles{Float64, 1000}}:
 -0.0183 ¬± 0.017
  0.0124 ¬± 0.017
  0.00951 ¬± 0.017
  0.00879 ¬± 0.017
 -0.0182 ¬± 0.017
 -0.0188 ¬± 0.017
 -0.000902 ¬± 0.017
  0.0143 ¬± 0.017
  0.0237 ¬± 0.017
  0.0319 ¬± 0.017
  ‚ãÆ
  0.00277 ¬± 0.017
 -0.0146 ¬± 0.017
 -0.00988 ¬± 0.017
  0.015 ¬± 0.017
  0.0214 ¬± 0.017
 -0.00509 ¬± 0.017
  0.00565 ¬± 0.017
 -0.00402 ¬± 0.017
 -0.0109 ¬± 0.017</pre><p>Construct each of the marginal posterior predictive distributions:</p><pre><code class="language-julia">predictor_marginal = MLJBase.predict(mach, X)
typeof(predictor_marginal)</code></pre><pre class="documenter-example-output">Vector{SossMLJ.SossMLJPredictor{SossMLJModel{SossMLJ.SossMLJPredictor, Soss.Model{NamedTuple{(:X, :s, :t), T} where T&lt;:Tuple, TypeEncoding(begin
    œÉ ~ HalfNormal(t)
    p = size(X, 2)
    Œ≤ ~ Normal(0, s) |&gt; iid(p)
    Œ∑ = X * Œ≤
    Œº = Œ∑
    y ~ For(eachindex(Œº)) do j
            Normal(Œº[j], œÉ)
        end
end), TypeEncoding(Main.ex-example-linear-regression)}, NamedTuple{(:s, :t), Tuple{Float64, Float64}}, typeof(Soss.dynamicHMC), Symbol, typeof(SossMLJ.default_transform)}, Vector{NamedTuple{(:œÉ, :Œ≤), Tuple{Float64, Vector{Float64}}}}, Soss.Model{NamedTuple{(:X, :œÉ, :Œ≤), T} where T&lt;:Tuple, TypeEncoding(begin
    Œ∑ = X * Œ≤
    Œº = Œ∑
    y ~ For(eachindex(Œº)) do j
            Normal(Œº[j], œÉ)
        end
end), TypeEncoding(Main.ex-example-linear-regression)}, NamedTuple{(:X, :s, :t), Tuple{Matrix{Float64}, Float64, Float64}}}} (alias for Array{SossMLJ.SossMLJPredictor{SossMLJModel{SossMLJ.SossMLJPredictor, Soss.Model{NamedTuple{(:X, :s, :t), T} where T&lt;:Tuple, TypeEncoding(begin
    œÉ ~ HalfNormal(t)
    p = size(X, 2)
    Œ≤ ~ Normal(0, s) |&gt; iid(p)
    Œ∑ = X * Œ≤
    Œº = Œ∑
    y ~ For(eachindex(Œº)) do j
            Normal(Œº[j], œÉ)
        end
end), TypeEncoding(Main.ex-example-linear-regression)}, NamedTuple{(:s, :t), Tuple{Float64, Float64}}, typeof(Soss.dynamicHMC), Symbol, typeof(SossMLJ.default_transform)}, Array{NamedTuple{(:œÉ, :Œ≤), Tuple{Float64, Array{Float64, 1}}}, 1}, Soss.Model{NamedTuple{(:X, :œÉ, :Œ≤), T} where T&lt;:Tuple, TypeEncoding(begin
    Œ∑ = X * Œ≤
    Œº = Œ∑
    y ~ For(eachindex(Œº)) do j
            Normal(Œº[j], œÉ)
        end
end), TypeEncoding(Main.ex-example-linear-regression)}, NamedTuple{(:X, :s, :t), Tuple{Array{Float64, 2}, Float64, Float64}}}, 1})</pre><p><code>predictor_marginal</code> has one element for each row in <code>X</code></p><pre><code class="language-julia">size(predictor_marginal)</code></pre><pre class="documenter-example-output">(1000,)</pre><p>Draw a single sample from each of the marginal posterior predictive distributions:</p><pre><code class="language-julia">only.(rand.(predictor_marginal))</code></pre><pre class="documenter-example-output">1000-element Vector{Float64}:
  0.22682334787603486
 -0.041743470127938685
  0.2760861262918858
  0.2926245418042688
  0.16910743954833735
  0.01957267100597679
  0.121535845700029
 -0.004145161390397827
  0.3248791853536659
  0.03784759656696146
  ‚ãÆ
 -0.024723503294231376
 -0.055263962408596504
  0.20894987786422767
 -0.17982510576681845
 -0.15121656452456847
  0.3502975934612336
 -0.07077371323331198
 -0.21943960686469688
 -0.03364899469198779</pre><p>Use cross-validation to evaluate the model with respect to the expected value of the root mean square error (RMSE)</p><pre><code class="language-julia">evaluate!(mach, resampling=CV(; nfolds = 6, shuffle = true), measure=rms_expected, operation=predict_particles)</code></pre><pre class="documenter-example-output">‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚îÇ _.measure        ‚îÇ _.measurement ‚îÇ _.per_fold                                ‚ãØ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚îÇ RMSExpected @704 ‚îÇ 0.0246        ‚îÇ [0.0238, 0.0249, 0.0243, 0.0251, 0.0245,  ‚ãØ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                                                                1 column omitted
_.per_observation = [missing]
_.fitted_params_per_fold = [ ‚Ä¶ ]
_.report_per_fold = [ ‚Ä¶ ]
</pre><p>Use cross-validation to evaluate the model with respect to the median of the root mean square error (RMSE)</p><pre><code class="language-julia">evaluate!(mach, resampling=CV(; nfolds = 6, shuffle = true), measure=rms_median, operation=predict_particles)</code></pre><pre class="documenter-example-output">‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚îÇ _.measure      ‚îÇ _.measurement ‚îÇ _.per_fold                                  ‚ãØ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚îÇ RMSMedian @156 ‚îÇ 0.0246        ‚îÇ [0.0248, 0.0253, 0.0243, 0.024, 0.0247, 0.0 ‚ãØ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                                                                1 column omitted
_.per_observation = [missing]
_.fitted_params_per_fold = [ ‚Ä¶ ]
_.report_per_fold = [ ‚Ä¶ ]
</pre><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">¬´ Home</a><a class="docs-footer-nextpage" href="../example-multinomial-logistic-regression/">Multinomial logistic regression ¬ª</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 29 March 2021 04:14">Monday 29 March 2021</span>. Using Julia version 1.6.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
